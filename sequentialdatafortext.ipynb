{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "#file_path = 'data.txt'\n",
        "\n",
        "# Read the text file\n",
        "#data = pd.read_csv(file_path, delimiter=\"\\t\", header=None)"
      ],
      "metadata": {
        "id": "nX3pbZd7SA08"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "#import random\n"
      ],
      "metadata": {
        "id": "dQHg2TgqVX64"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Data Preparation\n",
        "text_data = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"Here's the second one.\",\n",
        "    \"And the third one follows.\",\n",
        "    # Add more text data as needed\n",
        "]\n",
        "\n",
        "text = \" \".join(text_data)"
      ],
      "metadata": {
        "id": "CxRys0Q8Vcbt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Preprocessing\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in text_data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i + 1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]"
      ],
      "metadata": {
        "id": "cdhkrK5iVjoV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Build the LSTM Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=total_words, output_dim=100, input_length=max_sequence_length - 1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))"
      ],
      "metadata": {
        "id": "HdKjyVp9VnWN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Compile the Model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "WVYJG3vWVrkh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Training the Model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nw5-VV4VslM",
        "outputId": "c9a039f9-3123-4af2-e2e9-b4071136811e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 9.0821e-05 - accuracy: 1.0000\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 8.9943e-05 - accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 8.9076e-05 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 8.8242e-05 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 8.7397e-05 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.6617e-05 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.5837e-05 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 8.5078e-05 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.4341e-05 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 8.3604e-05 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.2911e-05 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.2217e-05 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.1535e-05 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 8.0874e-05 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.0223e-05 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.9627e-05 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 7.8988e-05 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 7.8381e-05 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 7.7775e-05 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 7.7200e-05 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 7.6648e-05 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 7.6073e-05 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 7.5477e-05 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 7.4935e-05 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 7.4426e-05 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 7.3884e-05 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.3353e-05 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 7.2855e-05 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 7.2356e-05 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 7.1836e-05 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 7.1349e-05 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 7.0872e-05 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 7.0417e-05 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.9929e-05 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 6.9474e-05 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.9019e-05 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.8585e-05 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.8141e-05 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 6.7708e-05 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.7263e-05 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.6852e-05 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.6429e-05 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.6028e-05 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.5605e-05 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.5194e-05 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 6.4825e-05 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 6.4392e-05 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 6.4013e-05 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 6.3622e-05 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 6.3254e-05 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 6.2864e-05 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.2474e-05 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 6.2105e-05 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.1769e-05 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 6.1369e-05 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 6.1054e-05 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.0675e-05 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.0339e-05 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.9960e-05 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.9635e-05 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 5.9320e-05 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 5.8963e-05 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 5.8638e-05 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 5.8302e-05 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 5.7977e-05 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.7662e-05 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.7381e-05 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 5.7066e-05 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.6731e-05 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.6438e-05 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 5.6113e-05 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 5.5799e-05 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 5.5528e-05 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 5.5213e-05 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 5.4943e-05 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.4650e-05 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.4357e-05 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 5.4076e-05 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 5.3805e-05 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 5.3512e-05 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 5.3241e-05 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 5.2949e-05 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 5.2689e-05 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 5.2428e-05 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.2168e-05 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 5.1898e-05 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 5.1616e-05 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 5.1388e-05 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 5.1128e-05 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 5.0857e-05 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 5.0597e-05 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 5.0337e-05 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 5.0131e-05 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 4.9871e-05 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 4.9622e-05 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.9383e-05 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 4.9123e-05 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 4.8896e-05 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 4.8701e-05 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 4.8452e-05 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x792cd8fc02b0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Text Generation\n",
        "seed_text = \"Here's the\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
        "\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
        "\n",
        "    seed_text += \" \" + predicted_word\n",
        "\n",
        "# Print the generated text\n",
        "print(f\"Generated Text: {seed_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp2pa54lVvki",
        "outputId": "abb333a5-b4b3-46e9-d9b0-f93bde507301"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: Here's the second one follows follows follows follows follows follows follows follows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Test the Accuracy (Perplexity)\n",
        "test_size = 0.2\n",
        "split_index = int(len(input_sequences) * (1 - test_size))\n",
        "X_test, y_test = X[split_index:], y[split_index:]\n",
        "\n",
        "cross_entropy_sum = 0\n",
        "word_count = 0\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    input_sequence = X_test[i:i+1]  # Provide a batch of size 1\n",
        "    target_word = y_test[i]\n",
        "    predicted = model.predict(input_sequence, verbose=0)\n",
        "    cross_entropy = -np.log(predicted[0][target_word])\n",
        "    cross_entropy_sum += cross_entropy\n",
        "    word_count += 1\n",
        "\n",
        "perplexity = np.exp(cross_entropy_sum / word_count)\n",
        "\n",
        "print(f\"Test Perplexity: {perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqmd5UycW_cl",
        "outputId": "d10e0aad-9bd9-4ddc-c77d-43bda14fe5b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Perplexity: 1.0000410099885186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say you have a language model that's trained on English text data,\n",
        "but it's a very poor model. When you evaluate this model on a test dataset,\n",
        "you might get a perplexity of 1,000 or even 10,000 or more. This means that,\n",
        "on average, the model is 1,000 or 10,000 times less certain about its predictions compared to a perfect model.\n",
        "\n",
        "In summary, a high perplexity value, such as 1,000 or higher,\n",
        " indicates a poor model that struggles to make accurate predictions,\n",
        " while a perplexity close to 1 suggests an excellent model with a high level of accuracy and confidence in its predictions."
      ],
      "metadata": {
        "id": "FT2-QlluZTu_"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFOqQprE4gis"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Step 1: Load and Preprocess Data\n",
        "data = pd.read_csv('spam_ham_dataset.csv')  # Replace with your dataset path\n",
        "data = shuffle(data)  # Shuffle the data to randomize class distribution\n",
        "text_data = data['text']\n",
        "labels = data['label']\n",
        "\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "sequences = tokenizer.texts_to_sequences(text_data)\n",
        "X = pad_sequences(sequences)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Step 2: Handling Class Imbalance (Optional)\n",
        "# You can apply techniques to address class imbalance, such as oversampling, undersampling, or using class weights.\n",
        "\n",
        "# Step 3: Split the Data into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Build the RNN Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128),\n",
        "    SimpleRNN(64),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the Model\n",
        "history = model.fit(X_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "# Step 7: Plot Accuracy and Loss Curves\n",
        "# You can visualize training and validation accuracy and loss using Matplotlib.\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 8: Predict for Sample Text\n",
        "# You can use the trained model to predict whether sample text is 'ham' or 'spam'.\n",
        "sample_text = [\"Your Amazon order is out for delivery.\"]\n",
        "sample_sequence = tokenizer.texts_to_sequences(sample_text)\n",
        "sample_sequence = pad_sequences(sample_sequence, maxlen=X.shape[1])  # Match sequence length with training data\n",
        "prediction = model.predict(sample_sequence)\n",
        "class_label = 'spam' if prediction > 0.5 else 'ham'\n",
        "print(f\"Sample Text: {sample_text[0]}\\nPredicted Label: {class_label}\")\n",
        "\n",
        "# Step 9: Hyperparameter Tuning (Optional)\n",
        "# You can fine-tune hyperparameters, such as the number of epochs, batch size, and model architecture, to optimize performance.\n"
      ]
    }
  ]
}
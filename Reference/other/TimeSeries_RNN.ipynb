{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cH_fWKD82FC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Step 1: Load and Preprocess Data\n",
        "data = pd.read_csv('time_series_data.csv')  # Replace with your dataset path\n",
        "X = data['feature'].values\n",
        "y = data['target'].values\n",
        "\n",
        "# Normalize the data\n",
        "X = (X - np.mean(X)) / np.std(X)\n",
        "\n",
        "# Step 2: Handling Class Imbalance (Optional)\n",
        "# You can apply over-sampling or under-sampling to handle class imbalance.\n",
        "# Example for over-sampling:\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X.reshape(-1, 1), y)\n",
        "\n",
        "# Example for under-sampling:\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X.reshape(-1, 1), y)\n",
        "\n",
        "# Step 3: Split the Data into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Build the RNN Model\n",
        "model = Sequential([\n",
        "    SimpleRNN(64, input_shape=(X_train.shape[1], 1)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Hyperparameter Tuning\n",
        "# You can perform hyperparameter tuning to optimize the model's performance.\n",
        "\n",
        "# Example of hyperparameter tuning:\n",
        "# hyperparameters = {'batch_size': [16, 32, 64], 'epochs': [10, 20, 30]}\n",
        "# best_accuracy = 0\n",
        "# best_model = None\n",
        "# for batch_size in hyperparameters['batch_size']:\n",
        "#     for epochs in hyperparameters['epochs']:\n",
        "#         tuned_model = Sequential([\n",
        "#             SimpleRNN(64, input_shape=(X_train.shape[1], 1)),\n",
        "#             Dense(1, activation='sigmoid')\n",
        "#         ])\n",
        "#         tuned_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#         history = tuned_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=0)\n",
        "#         accuracy = max(history.history['val_accuracy'])\n",
        "#         if accuracy > best_accuracy:\n",
        "#             best_accuracy = accuracy\n",
        "#             best_model = tuned_model\n",
        "# model = best_model  # Use the best-tuned model\n",
        "\n",
        "# Step 6: Train the Model\n",
        "history = model.fit(X_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Step 7: Evaluate the Model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Step 8: Plot Accuracy and Loss Curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 9: Predict for Sample Data\n",
        "sample_data = np.array([0.1, 0.2, 0.3, 0.4])  # Replace with your own sample data\n",
        "sample_data = (sample_data - np.mean(sample_data)) / np.std(sample_data)  # Normalize the sample data\n",
        "sample_data = sample_data.reshape(1, -1, 1)\n",
        "prediction = model.predict(sample_data)\n",
        "predicted_class = 'Positive' if prediction > 0.5 else 'Negative'\n",
        "print(f'Sample Data: {sample_data[0, :, 0]}')\n",
        "print(f'Predicted Class: {predicted_class}')\n"
      ]
    }
  ]
}
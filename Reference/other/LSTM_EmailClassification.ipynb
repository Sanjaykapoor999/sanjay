{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPHg8x414UZK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load and Preprocess Data\n",
        "# Assume you have a dataset with 'text' and 'label' columns.\n",
        "data = pd.read_csv('spam_ham_dataset.csv')  # Replace with your dataset path\n",
        "text_data = data['text']\n",
        "labels = data['label']\n",
        "\n",
        "# Text preprocessing\n",
        "max_words = 10000  # Maximum number of unique words to consider\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "sequences = tokenizer.texts_to_sequences(text_data)\n",
        "X = pad_sequences(sequences)\n",
        "y = to_categorical(labels, num_classes=2)\n",
        "\n",
        "# Step 2: Handle Class Imbalance (if needed)\n",
        "# If your dataset is imbalanced, consider oversampling, undersampling, or using weighted classes.\n",
        "\n",
        "# Step 3: Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Build the RNN Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=X.shape[1]),\n",
        "    LSTM(64),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Hyperparameter Tuning (optional)\n",
        "# Experiment with different hyperparameters such as the number of LSTM units, batch size, and learning rate.\n",
        "\n",
        "# Step 6: Train the Model\n",
        "history = model.fit(X_train, y_train, batch_size=64, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Step 7: Plot Training and Validation Accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "# Step 9: Make Predictions\n",
        "# You can use the model to predict whether a sample text is spam or ham.\n",
        "sample_text = [\"Get a free iPhone now!\"]\n",
        "sample_sequence = tokenizer.texts_to_sequences(sample_text)\n",
        "sample_X = pad_sequences(sample_sequence, maxlen=X.shape[1])\n",
        "prediction = model.predict(sample_X)\n",
        "\n",
        "# Step 10: Display the Prediction\n",
        "# The prediction will be a probability distribution; you can use a threshold to classify as spam or ham.\n",
        "threshold = 0.5\n",
        "if prediction[0][1] > threshold:\n",
        "    print(\"Predicted: Spam\")\n",
        "else:\n",
        "    print(\"Predicted: Ham\")\n"
      ]
    }
  ]
}
# -*- coding: utf-8 -*-
"""MLOM_Lab8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q6VxDY5GXmnQUtOe84MA1zuQ3f4nUy0w
"""

import numpy as np
import random
#modifiy version of list, provide list,dictonary linke compound library
from collections import deque
import gym
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense, Embedding, Reshape
from tensorflow.keras.optimizers import Adam

#create the Taxi-v3 enviroment
enviroment = gym.make("Taxi-v3").env
enviroment.reset()
#ender the enviroment
enviroment.render()

print(enviroment.observation_space.n)
print(enviroment.action_space.n)

class Agent:
 def __init__(self, enviroment, optimizer):

 # Initialize atributes
  self._state_size = enviroment.observation_space.n
  self._action_size = enviroment.action_space.n
  self._optimizer = optimizer

  self.expirience_replay = deque(maxlen=2000)

  # Initialize discount and exploration rate
  self.gamma = 0.6
  self.epsilon = 0.1

  # Build networks
  self.q_network = self._build_compile_model()
  self.target_network = self._build_compile_model()
  self.alighn_target_model()

 #in deep learning we do copy the weight in q_network and set those weight to target network...this code we did it
  def alighn_target_model(self):
    self.target_network.set_weights(self.q_network.get_weights())

  def store(self, state, action, reward, next_state, terminated):
    self.expirience_replay.append((state, action, reward, next_state, terminated))

  def _build_compile_model(self):
    model = Sequential()
    #embedding - reduce diamentional (500 ->10),only represent not removing
    model.add(Embedding(self._state_size, 10, input_length=1))
    model.add(Reshape((10,)))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(self._action_size, activation='linear'))

    model.compile(loss='mse', optimizer=self._optimizer)
    return model

 def act(self, state):
  #check random is deetween 0 or 1
  if np.random.rand() <= self.epsilon:
    return enviroment.action_space.sample()

    q_values = self.q_network.predict(state)
    return np.argmax(q_values[0])

 def retrain(self, batch_size):
  minibatch = random.sample(self.expirience_replay, batch_size)

  for state, action, reward, next_state, terminated in minibatch:
    target = self.q_network.predict(state)

    if terminated:
      target[0][action] = reward
    else:
      t = self.target_network.predict(next_state)
      target[0][action] = reward + self.gamma * np.amax(t)

    self.q_network.fit(state, target, epochs=1, verbose=0)

#access the target networks,s weights
target_weights = agent.target_netwirk.get_weights()
target_weights

optimizer = Adam(learning_rate=0.01)
agent = Agent(enviroment, optimizer)
agent.q_network.summary()

batch_size = 32
num_of_episodes = 100
timesteps_per_episode = 100
for e in range(0, num_of_episodes):
 # Reset the enviroment
 state = enviroment.reset()
 state = np.reshape(state, [1, 1])

 # Initialize variables
 reward = 0
 terminated = False

 for timestep in range(timesteps_per_episode):
 # Run Action
  action = agent.act(state)

 # Take action
 next_state, reward, terminated, info = enviroment.step(action)
 next_state = np.reshape(next_state, [1, 1])
 agent.store(state, action, reward, next_state, terminated)

 state = next_state

 if terminated:
  agent.alighn_target_model()
 break

 if len(agent.expirience_replay) > batch_size:
  agent.retrain(batch_size)


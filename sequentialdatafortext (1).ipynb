{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZ0A4Yiz50nrLwfKbFWJ+9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#import pandas as pd\n","#file_path = 'data.txt'\n","\n","# Read the text file\n","#data = pd.read_csv(file_path, delimiter=\"\\t\", header=None)\n","\n","\n","# Load ascii text and covert to lowercase\n","\n","##filename = \"data.txt\"\n","#raw_text = open(filename , 'r' , encoding='utf-8').read()\n","#raw_text = raw_text.lower()"],"metadata":{"id":"nX3pbZd7SA08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRe_ok4-beVL","executionInfo":{"status":"ok","timestamp":1697341397486,"user_tz":-330,"elapsed":23900,"user":{"displayName":"Nuha Nafas","userId":"04210910949617463014"}},"outputId":"76898798-09e0-4008-e069-31d223ba0f5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.optimizers import Adam\n","#from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n","#import matplotlib.pyplot as plt\n","#import seaborn as sns\n","#from sklearn.metrics import confusion_matrix\n","#import random\n"],"metadata":{"id":"dQHg2TgqVX64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Data Preparation\n","text_data = [\n","    \"This is the first sentence.\",\n","    \"Here's the second one.\",\n","    \"And the third one follows.\",\n","    # Add more text data as needed\n","]\n","\n","text = \" \".join(text_data)"],"metadata":{"id":"CxRys0Q8Vcbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 2: Data Preprocessing\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","total_words = len(tokenizer.word_index) + 1\n","\n","input_sequences = []\n","for line in text_data:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i + 1]\n","        input_sequences.append(n_gram_sequence)\n","\n","max_sequence_length = max([len(seq) for seq in input_sequences])\n","input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n","\n","X, y = input_sequences[:, :-1], input_sequences[:, -1]"],"metadata":{"id":"cdhkrK5iVjoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Build the LSTM Model\n","model = Sequential()\n","model.add(Embedding(input_dim=total_words, output_dim=100, input_length=max_sequence_length - 1))\n","model.add(LSTM(150))\n","model.add(Dense(total_words, activation='softmax'))"],"metadata":{"id":"HdKjyVp9VnWN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Compile the Model\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n"],"metadata":{"id":"WVYJG3vWVrkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Training the Model\n","model.fit(X, y, epochs=100, verbose=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nw5-VV4VslM","executionInfo":{"status":"ok","timestamp":1697341430016,"user_tz":-330,"elapsed":8351,"user":{"displayName":"Nuha Nafas","userId":"04210910949617463014"}},"outputId":"fe61ea7a-de09-4d50-9773-2fcef80a0f49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1/1 [==============================] - 3s 3s/step - loss: 2.4870 - accuracy: 0.0000e+00\n","Epoch 2/100\n","1/1 [==============================] - 0s 19ms/step - loss: 2.3732 - accuracy: 0.2727\n","Epoch 3/100\n","1/1 [==============================] - 0s 20ms/step - loss: 2.1728 - accuracy: 0.2727\n","Epoch 4/100\n","1/1 [==============================] - 0s 19ms/step - loss: 1.8578 - accuracy: 0.2727\n","Epoch 5/100\n","1/1 [==============================] - 0s 19ms/step - loss: 1.8629 - accuracy: 0.2727\n","Epoch 6/100\n","1/1 [==============================] - 0s 27ms/step - loss: 1.4956 - accuracy: 0.5455\n","Epoch 7/100\n","1/1 [==============================] - 0s 26ms/step - loss: 1.3158 - accuracy: 0.7273\n","Epoch 8/100\n","1/1 [==============================] - 0s 21ms/step - loss: 1.1142 - accuracy: 0.9091\n","Epoch 9/100\n","1/1 [==============================] - 0s 28ms/step - loss: 0.8748 - accuracy: 1.0000\n","Epoch 10/100\n","1/1 [==============================] - 0s 32ms/step - loss: 0.6681 - accuracy: 0.9091\n","Epoch 11/100\n","1/1 [==============================] - 0s 28ms/step - loss: 0.5010 - accuracy: 0.8182\n","Epoch 12/100\n","1/1 [==============================] - 0s 30ms/step - loss: 0.3805 - accuracy: 0.9091\n","Epoch 13/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.3066 - accuracy: 0.9091\n","Epoch 14/100\n","1/1 [==============================] - 0s 22ms/step - loss: 0.2450 - accuracy: 1.0000\n","Epoch 15/100\n","1/1 [==============================] - 0s 21ms/step - loss: 0.1960 - accuracy: 1.0000\n","Epoch 16/100\n","1/1 [==============================] - 0s 21ms/step - loss: 0.1587 - accuracy: 1.0000\n","Epoch 17/100\n","1/1 [==============================] - 0s 22ms/step - loss: 0.1154 - accuracy: 1.0000\n","Epoch 18/100\n","1/1 [==============================] - 0s 21ms/step - loss: 0.0771 - accuracy: 1.0000\n","Epoch 19/100\n","1/1 [==============================] - 0s 23ms/step - loss: 0.0497 - accuracy: 1.0000\n","Epoch 20/100\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0382 - accuracy: 1.0000\n","Epoch 21/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0264 - accuracy: 1.0000\n","Epoch 22/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0172 - accuracy: 1.0000\n","Epoch 23/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0113 - accuracy: 1.0000\n","Epoch 24/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0073 - accuracy: 1.0000\n","Epoch 25/100\n","1/1 [==============================] - 0s 23ms/step - loss: 0.0048 - accuracy: 1.0000\n","Epoch 26/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0034 - accuracy: 1.0000\n","Epoch 27/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0026 - accuracy: 1.0000\n","Epoch 28/100\n","1/1 [==============================] - 0s 21ms/step - loss: 0.0021 - accuracy: 1.0000\n","Epoch 29/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 30/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 31/100\n","1/1 [==============================] - 0s 21ms/step - loss: 0.0012 - accuracy: 1.0000\n","Epoch 32/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 33/100\n","1/1 [==============================] - 0s 21ms/step - loss: 9.8883e-04 - accuracy: 1.0000\n","Epoch 34/100\n","1/1 [==============================] - 0s 21ms/step - loss: 9.1257e-04 - accuracy: 1.0000\n","Epoch 35/100\n","1/1 [==============================] - 0s 22ms/step - loss: 8.5181e-04 - accuracy: 1.0000\n","Epoch 36/100\n","1/1 [==============================] - 0s 30ms/step - loss: 7.9869e-04 - accuracy: 1.0000\n","Epoch 37/100\n","1/1 [==============================] - 0s 26ms/step - loss: 7.4803e-04 - accuracy: 1.0000\n","Epoch 38/100\n","1/1 [==============================] - 0s 26ms/step - loss: 6.9731e-04 - accuracy: 1.0000\n","Epoch 39/100\n","1/1 [==============================] - 0s 23ms/step - loss: 6.4591e-04 - accuracy: 1.0000\n","Epoch 40/100\n","1/1 [==============================] - 0s 33ms/step - loss: 5.9477e-04 - accuracy: 1.0000\n","Epoch 41/100\n","1/1 [==============================] - 0s 30ms/step - loss: 5.4535e-04 - accuracy: 1.0000\n","Epoch 42/100\n","1/1 [==============================] - 0s 30ms/step - loss: 4.9908e-04 - accuracy: 1.0000\n","Epoch 43/100\n","1/1 [==============================] - 0s 27ms/step - loss: 4.5687e-04 - accuracy: 1.0000\n","Epoch 44/100\n","1/1 [==============================] - 0s 29ms/step - loss: 4.1924e-04 - accuracy: 1.0000\n","Epoch 45/100\n","1/1 [==============================] - 0s 32ms/step - loss: 3.8610e-04 - accuracy: 1.0000\n","Epoch 46/100\n","1/1 [==============================] - 0s 30ms/step - loss: 3.5709e-04 - accuracy: 1.0000\n","Epoch 47/100\n","1/1 [==============================] - 0s 28ms/step - loss: 3.3189e-04 - accuracy: 1.0000\n","Epoch 48/100\n","1/1 [==============================] - 0s 37ms/step - loss: 3.0999e-04 - accuracy: 1.0000\n","Epoch 49/100\n","1/1 [==============================] - 0s 30ms/step - loss: 2.9090e-04 - accuracy: 1.0000\n","Epoch 50/100\n","1/1 [==============================] - 0s 36ms/step - loss: 2.7423e-04 - accuracy: 1.0000\n","Epoch 51/100\n","1/1 [==============================] - 0s 27ms/step - loss: 2.5958e-04 - accuracy: 1.0000\n","Epoch 52/100\n","1/1 [==============================] - 0s 34ms/step - loss: 2.4668e-04 - accuracy: 1.0000\n","Epoch 53/100\n","1/1 [==============================] - 0s 25ms/step - loss: 2.3532e-04 - accuracy: 1.0000\n","Epoch 54/100\n","1/1 [==============================] - 0s 30ms/step - loss: 2.2514e-04 - accuracy: 1.0000\n","Epoch 55/100\n","1/1 [==============================] - 0s 27ms/step - loss: 2.1611e-04 - accuracy: 1.0000\n","Epoch 56/100\n","1/1 [==============================] - 0s 23ms/step - loss: 2.0796e-04 - accuracy: 1.0000\n","Epoch 57/100\n","1/1 [==============================] - 0s 20ms/step - loss: 2.0061e-04 - accuracy: 1.0000\n","Epoch 58/100\n","1/1 [==============================] - 0s 21ms/step - loss: 1.9400e-04 - accuracy: 1.0000\n","Epoch 59/100\n","1/1 [==============================] - 0s 20ms/step - loss: 1.8798e-04 - accuracy: 1.0000\n","Epoch 60/100\n","1/1 [==============================] - 0s 20ms/step - loss: 1.8249e-04 - accuracy: 1.0000\n","Epoch 61/100\n","1/1 [==============================] - 0s 21ms/step - loss: 1.7744e-04 - accuracy: 1.0000\n","Epoch 62/100\n","1/1 [==============================] - 0s 20ms/step - loss: 1.7286e-04 - accuracy: 1.0000\n","Epoch 63/100\n","1/1 [==============================] - 0s 22ms/step - loss: 1.6857e-04 - accuracy: 1.0000\n","Epoch 64/100\n","1/1 [==============================] - 0s 22ms/step - loss: 1.6464e-04 - accuracy: 1.0000\n","Epoch 65/100\n","1/1 [==============================] - 0s 21ms/step - loss: 1.6096e-04 - accuracy: 1.0000\n","Epoch 66/100\n","1/1 [==============================] - 0s 21ms/step - loss: 1.5756e-04 - accuracy: 1.0000\n","Epoch 67/100\n","1/1 [==============================] - 0s 20ms/step - loss: 1.5433e-04 - accuracy: 1.0000\n","Epoch 68/100\n","1/1 [==============================] - 0s 28ms/step - loss: 1.5137e-04 - accuracy: 1.0000\n","Epoch 69/100\n","1/1 [==============================] - 0s 27ms/step - loss: 1.4853e-04 - accuracy: 1.0000\n","Epoch 70/100\n","1/1 [==============================] - 0s 26ms/step - loss: 1.4588e-04 - accuracy: 1.0000\n","Epoch 71/100\n","1/1 [==============================] - 0s 37ms/step - loss: 1.4334e-04 - accuracy: 1.0000\n","Epoch 72/100\n","1/1 [==============================] - 0s 33ms/step - loss: 1.4098e-04 - accuracy: 1.0000\n","Epoch 73/100\n","1/1 [==============================] - 0s 23ms/step - loss: 1.3873e-04 - accuracy: 1.0000\n","Epoch 74/100\n","1/1 [==============================] - 0s 28ms/step - loss: 1.3657e-04 - accuracy: 1.0000\n","Epoch 75/100\n","1/1 [==============================] - 0s 29ms/step - loss: 1.3452e-04 - accuracy: 1.0000\n","Epoch 76/100\n","1/1 [==============================] - 0s 32ms/step - loss: 1.3259e-04 - accuracy: 1.0000\n","Epoch 77/100\n","1/1 [==============================] - 0s 31ms/step - loss: 1.3072e-04 - accuracy: 1.0000\n","Epoch 78/100\n","1/1 [==============================] - 0s 34ms/step - loss: 1.2892e-04 - accuracy: 1.0000\n","Epoch 79/100\n","1/1 [==============================] - 0s 33ms/step - loss: 1.2722e-04 - accuracy: 1.0000\n","Epoch 80/100\n","1/1 [==============================] - 0s 29ms/step - loss: 1.2560e-04 - accuracy: 1.0000\n","Epoch 81/100\n","1/1 [==============================] - 0s 36ms/step - loss: 1.2402e-04 - accuracy: 1.0000\n","Epoch 82/100\n","1/1 [==============================] - 0s 25ms/step - loss: 1.2250e-04 - accuracy: 1.0000\n","Epoch 83/100\n","1/1 [==============================] - 0s 25ms/step - loss: 1.2103e-04 - accuracy: 1.0000\n","Epoch 84/100\n","1/1 [==============================] - 0s 24ms/step - loss: 1.1960e-04 - accuracy: 1.0000\n","Epoch 85/100\n","1/1 [==============================] - 0s 29ms/step - loss: 1.1828e-04 - accuracy: 1.0000\n","Epoch 86/100\n","1/1 [==============================] - 0s 25ms/step - loss: 1.1696e-04 - accuracy: 1.0000\n","Epoch 87/100\n","1/1 [==============================] - 0s 32ms/step - loss: 1.1567e-04 - accuracy: 1.0000\n","Epoch 88/100\n","1/1 [==============================] - 0s 27ms/step - loss: 1.1444e-04 - accuracy: 1.0000\n","Epoch 89/100\n","1/1 [==============================] - 0s 34ms/step - loss: 1.1326e-04 - accuracy: 1.0000\n","Epoch 90/100\n","1/1 [==============================] - 0s 32ms/step - loss: 1.1212e-04 - accuracy: 1.0000\n","Epoch 91/100\n","1/1 [==============================] - 0s 21ms/step - loss: 1.1098e-04 - accuracy: 1.0000\n","Epoch 92/100\n","1/1 [==============================] - 0s 20ms/step - loss: 1.0991e-04 - accuracy: 1.0000\n","Epoch 93/100\n","1/1 [==============================] - 0s 22ms/step - loss: 1.0886e-04 - accuracy: 1.0000\n","Epoch 94/100\n","1/1 [==============================] - 0s 22ms/step - loss: 1.0782e-04 - accuracy: 1.0000\n","Epoch 95/100\n","1/1 [==============================] - 0s 22ms/step - loss: 1.0680e-04 - accuracy: 1.0000\n","Epoch 96/100\n","1/1 [==============================] - 0s 21ms/step - loss: 1.0586e-04 - accuracy: 1.0000\n","Epoch 97/100\n","1/1 [==============================] - 0s 24ms/step - loss: 1.0491e-04 - accuracy: 1.0000\n","Epoch 98/100\n","1/1 [==============================] - 0s 26ms/step - loss: 1.0397e-04 - accuracy: 1.0000\n","Epoch 99/100\n","1/1 [==============================] - 0s 25ms/step - loss: 1.0303e-04 - accuracy: 1.0000\n","Epoch 100/100\n","1/1 [==============================] - 0s 26ms/step - loss: 1.0218e-04 - accuracy: 1.0000\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7bf4cf68fc10>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["\n","# Step 6: Text Generation\n","seed_text = \"Here's the\"\n","next_words = 10\n","\n","for _ in range(next_words):\n","    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n","\n","    predicted = model.predict(token_list, verbose=0)\n","    predicted_word = tokenizer.index_word[np.argmax(predicted)]\n","\n","    seed_text += \" \" + predicted_word\n","\n","# Print the generated text\n","print(f\"Generated Text: {seed_text}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hp2pa54lVvki","executionInfo":{"status":"ok","timestamp":1697341437731,"user_tz":-330,"elapsed":1594,"user":{"displayName":"Nuha Nafas","userId":"04210910949617463014"}},"outputId":"1d0e96a0-423c-4c90-a4bf-756a67a61a1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Text: Here's the second one one follows follows follows follows follows follows follows\n"]}]},{"cell_type":"code","source":["# Step 7: Test the Accuracy (Perplexity)\n","test_size = 0.2\n","split_index = int(len(input_sequences) * (1 - test_size))\n","X_test, y_test = X[split_index:], y[split_index:]\n","\n","cross_entropy_sum = 0\n","word_count = 0\n","\n","for i in range(len(X_test)):\n","    input_sequence = X_test[i:i+1]  # Provide a batch of size 1\n","    target_word = y_test[i]\n","    predicted = model.predict(input_sequence, verbose=0)\n","    cross_entropy = -np.log(predicted[0][target_word])\n","    cross_entropy_sum += cross_entropy\n","    word_count += 1\n","\n","perplexity = np.exp(cross_entropy_sum / word_count)\n","\n","print(f\"Test Perplexity: {perplexity}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gqmd5UycW_cl","executionInfo":{"status":"ok","timestamp":1697341439147,"user_tz":-330,"elapsed":816,"user":{"displayName":"Nuha Nafas","userId":"04210910949617463014"}},"outputId":"3bf7011e-fb89-4a9f-f4c6-c42a695fd334"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Perplexity: 1.0001096849108224\n"]}]},{"cell_type":"markdown","source":["Let's say you have a language model that's trained on English text data,\n","but it's a very poor model. When you evaluate this model on a test dataset,\n","you might get a perplexity of 1,000 or even 10,000 or more. This means that,\n","on average, the model is 1,000 or 10,000 times less certain about its predictions compared to a perfect model.\n","\n","In summary, a high perplexity value, such as 1,000 or higher,\n"," indicates a poor model that struggles to make accurate predictions,\n"," while a perplexity close to 1 suggests an excellent model with a high level of accuracy and confidence in its predictions."],"metadata":{"id":"FT2-QlluZTu_"}}]}